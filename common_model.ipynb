{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "13DWFcqlzqXJ6bIhz_t237FTmIO3V98Av",
      "authorship_tag": "ABX9TyPmrdARFf8sV+OCjcOtdW6A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adith-blusim/BP-/blob/main/common_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GfoKjM1wpQyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUf5AaziSJRu"
      },
      "outputs": [],
      "source": [
        "#Deleting all values of \"sys\" > 140 and 'dias' values > 100:\n",
        "#seconds data\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/blusim/roi_updated_data.csv\")\n",
        "df = df.dropna()\n",
        "df = df.reset_index(drop=True)\n",
        "df = df.drop(['Unix Timestamp','RESP','ECG','SPO2','PR'],axis=1)\n",
        "\n",
        "def extract_nibp_values(value):\n",
        "    # Remove spaces and extra brackets, extract the digits\n",
        "    cleaned_value = ''.join(filter(str.isdigit, value.replace(' ', '').replace('(', '').replace(')', '')))\n",
        "    sys_value = cleaned_value[:3]\n",
        "    dias_value = cleaned_value[3:5]\n",
        "    return sys_value, dias_value\n",
        "\n",
        "# Apply the function to \"NIBP\" column and create new columns \"sys\" and \"dias\"\n",
        "df['sys'], df['dias'] = zip(*df['NIBP'].apply(extract_nibp_values))\n",
        "\n",
        "rdf = pd.read_csv(\"/content/drive/MyDrive/blusim/mat_15.csv\")\n",
        "rdf['Timestamp'] = pd.to_datetime(rdf['Timestamp'], format='%H:%M:%S')\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%H:%M:%S')\n",
        "rdf['Timestamp'] = rdf['Timestamp'].dt.strftime('%H:%M:%S')\n",
        "\n",
        "df.set_index('Timestamp', inplace=True)\n",
        "rdf = rdf.groupby('Timestamp').agg(lambda x: x.tolist()).reset_index()\n",
        "rdf.set_index('Timestamp', inplace=True)\n",
        "df=df.resample('S').ffill()\n",
        "df.index = df.index.strftime('%H:%M:%S')\n",
        "\n",
        "ldf = pd.merge(rdf,df, left_index=True, right_index=True)\n",
        "ldf[['sys', 'dias']] = ldf[['sys', 'dias']].replace('', np.nan)\n",
        "ldf.dropna(how='any', inplace=True)\n",
        "ldf['sys'] = pd.to_numeric(ldf['sys'], errors='coerce')\n",
        "ldf['dias'] = pd.to_numeric(ldf['dias'], errors='coerce')\n",
        "ldf = ldf.drop(ldf[ldf['sys'].gt(140)].index)\n",
        "ldf = ldf.drop(ldf[ldf['dias'].gt(100)].index)\n",
        "ldf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Interpolation before signal processing\n",
        "\n",
        "from scipy.interpolate import PchipInterpolator\n",
        "\n",
        "def interpolate_to_120(arr):\n",
        "\n",
        "    pchip_interpolator = PchipInterpolator(np.arange(len(arr)), arr)\n",
        "    x_interp = np.linspace(0, len(arr) - 1, 120)\n",
        "    y_interp = pchip_interpolator(x_interp)\n",
        "    return y_interp.tolist()\n",
        "\n",
        "df_interpolated = ldf[['Raw_Value_ch1','Raw_Value_ch2','Raw_Value_ch3','Raw_Value_ch4']].applymap(interpolate_to_120)\n",
        "df_interpolated"
      ],
      "metadata": {
        "id": "gJrP_Dn4qEIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install EMD-signal\n",
        "from PyEMD import EMD\n",
        "from PyEMD import EEMD\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import butter, lfilter, hilbert\n",
        "# Sample rate and desired cutoff frequencies (in Hz)\n",
        "fs = 138.0\n",
        "lowcut = 0.5\n",
        "highcut = 6.0\n",
        "def butter_bandpass(lowcut, highcut, fs, order=3):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=3):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "# Apply the Butterworth band-pass filter to the data\n",
        "bdf = pd.DataFrame()\n",
        "bdf['buttered_channel1'] = [butter_bandpass_filter(data, lowcut, highcut, fs, order=3) for data in df_interpolated['Raw_Value_ch1']]\n",
        "bdf['buttered_channel2'] = [butter_bandpass_filter(data, lowcut, highcut, fs, order=3) for data in df_interpolated['Raw_Value_ch2']]\n",
        "bdf['buttered_channel3'] = [butter_bandpass_filter(data, lowcut, highcut, fs, order=3) for data in df_interpolated['Raw_Value_ch3']]\n",
        "bdf['buttered_channel4'] = [butter_bandpass_filter(data, lowcut, highcut, fs, order=3) for data in df_interpolated['Raw_Value_ch4']]\n",
        "\n",
        "eemd = EMD()\n",
        "imf1c1 = []\n",
        "imf1c2 = []\n",
        "imf1c3 = []\n",
        "imf1c4 = []\n",
        "\n",
        "for i in bdf['buttered_channel1']:\n",
        "    time = np.arange(0, len(i)) / fs\n",
        "    IMFs = eemd.emd(i, time)\n",
        "    i1 = IMFs[0]\n",
        "    imf1c1.append(i1)\n",
        "\n",
        "for i in bdf['buttered_channel2']:\n",
        "    time = np.arange(0, len(i)) / fs\n",
        "    IMFs = eemd.emd(i, time)\n",
        "    i1 = IMFs[0]\n",
        "    imf1c2.append(i1)\n",
        "\n",
        "for i in bdf['buttered_channel3']:\n",
        "    time = np.arange(0, len(i)) / fs\n",
        "    IMFs = eemd.emd(i, time)\n",
        "    i1 = IMFs[0]\n",
        "    imf1c3.append(i1)\n",
        "\n",
        "for i in bdf['buttered_channel4']:\n",
        "    time = np.arange(0, len(i)) / fs\n",
        "    IMFs = eemd.emd(i, time)\n",
        "    i1 = IMFs[0]\n",
        "    imf1c4.append(i1)\n",
        "\n",
        "ins_phase_c1 = []\n",
        "ins_phase_c2 = []\n",
        "ins_phase_c3 = []\n",
        "ins_phase_c4 = []\n",
        "\n",
        "for i in imf1c1:\n",
        "    analytic_signal = hilbert(i)\n",
        "    instantaneous_phase = np.angle(analytic_signal)\n",
        "    ins_phase_c1.append(instantaneous_phase)\n",
        "\n",
        "for i in imf1c2:\n",
        "    analytic_signal = hilbert(i)\n",
        "    instantaneous_phase = np.angle(analytic_signal)\n",
        "    ins_phase_c2.append(instantaneous_phase)\n",
        "\n",
        "for i in imf1c3:\n",
        "    analytic_signal = hilbert(i)\n",
        "    instantaneous_phase = np.angle(analytic_signal)\n",
        "    ins_phase_c3.append(instantaneous_phase)\n",
        "\n",
        "for i in imf1c4:\n",
        "    analytic_signal = hilbert(i)\n",
        "    instantaneous_phase = np.angle(analytic_signal)\n",
        "    ins_phase_c4.append(instantaneous_phase)\n",
        "\n"
      ],
      "metadata": {
        "id": "E5ylmjRbS3-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Interpolation after signal processing\n",
        "\n",
        "from scipy.interpolate import PchipInterpolator\n",
        "\n",
        "def interpolate_to_120(arr):\n",
        "\n",
        "    pchip_interpolator = PchipInterpolator(np.arange(len(arr)), arr)\n",
        "    x_interp = np.linspace(0, len(arr) - 1, 120)\n",
        "    y_interp = pchip_interpolator(x_interp)\n",
        "    return y_interp.tolist()\n",
        "\n",
        "ipc1 = [interpolate_to_120(lst) for lst in ins_phase_c1]\n",
        "ipc2 = [interpolate_to_120(lst) for lst in ins_phase_c2]\n",
        "ipc3 = [interpolate_to_120(lst) for lst in ins_phase_c3]\n",
        "ipc4 = [interpolate_to_120(lst) for lst in ins_phase_c4]\n",
        "\n",
        "\n",
        "print(max(len(x) for x in ipc1))"
      ],
      "metadata": {
        "id": "T6EARt8VG1LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standarizing array-size in each cell to be the same as the size of the smallest array\n",
        "minlen = (min(len(x) for x in ins_phase_c1))\n",
        "ipc1 = [lst[:minlen] for lst in ins_phase_c1 if len(lst) >= minlen]\n",
        "ipc2 = [lst[:minlen] for lst in ins_phase_c2 if len(lst) >= minlen]\n",
        "ipc3 = [lst[:minlen] for lst in ins_phase_c3 if len(lst) >= minlen]\n",
        "ipc4 = [lst[:minlen] for lst in ins_phase_c4 if len(lst) >= minlen]\n",
        "\n",
        "print(max(len(x) for x in ipc1))"
      ],
      "metadata": {
        "id": "d4MTEQV0UWVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "merged_ipc = np.stack((ipc1,ipc2,ipc3,ipc4), axis=1)\n",
        "merged_ipc = np.swapaxes(merged_ipc, 1, 2)\n",
        "print(merged_ipc.shape)\n",
        "\n",
        "X_train = merged_ipc\n",
        "Y_train = ldf[['sys', 'dias']].to_numpy().astype('float32')\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "fjfgUNTvVdqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Flatten, Dense, BatchNormalization\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "input_shape =  (120, 4)\n",
        "model = Sequential()\n",
        "\n",
        "#first Conv layer\n",
        "model.add(Conv1D(filters=100, kernel_size=21, strides=1, input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.ReLU())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "#second Conv layer\n",
        "model.add(Conv1D(filters=200, kernel_size=5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.ReLU())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "#third Conv layer\n",
        "model.add(Conv1D(filters=300, kernel_size=5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.ReLU())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "#GlobalAveragePooling1D layer\n",
        "model.add(GlobalAveragePooling1D())\n",
        "#Final dense layer\n",
        "model.add(Dense(2, activation='linear'))\n",
        "\n",
        "model.compile(loss=tf.keras.losses.mean_squared_error, optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "model.summary()\n",
        "history = model.fit(X_train, Y_train, epochs=50, validation_data=(X_test, Y_test), batch_size=64)"
      ],
      "metadata": {
        "id": "yQbm5ATtVeM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test)\n",
        "Y_pred = []\n",
        "yf = pd.DataFrame(predictions, columns=['predicted_output_1', 'predicted_output_2'])\n",
        "yf[['true_output_1', 'true_output_2']] = pd.DataFrame(Y_test, columns=['true_output_1', 'true_output_2'])\n",
        "# yf = pd.DataFrame(Y_pred, columns=['predicted'])\n",
        "yf\n"
      ],
      "metadata": {
        "id": "pW5Z5ermXurU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "absolute_percentage_error = np.abs((yf[['true_output_1', 'true_output_2']].values - yf[['predicted_output_1', 'predicted_output_2']].values) / yf[['true_output_1', 'true_output_2']].values)\n",
        "\n",
        "# Calculate accuracy for each row\n",
        "accuracy_per_row = 1 - absolute_percentage_error\n",
        "\n",
        "# Set your accuracy threshold 90%\n",
        "accuracy_threshold = 0.9\n",
        "\n",
        "# Filter rows where both predictions have accuracy >= accuracy_threshold\n",
        "accurate_rows = yf[(accuracy_per_row >= accuracy_threshold).all(axis=1)]\n",
        "\n",
        "# Print the DataFrame with only accurate rows\n",
        "print(\"Accurate Rows:\")\n",
        "print(accurate_rows)"
      ],
      "metadata": {
        "id": "C3DvykEiGLEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Training loss\n",
        "plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
        "\n",
        "# Validation loss\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Performance')\n",
        "plt.legend()\n",
        "\n",
        "# Adding grid for better readability\n",
        "plt.grid(True)\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LTz5w5UdkW0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['axes.facecolor'] = 'black'\n",
        "plt.rcParams['figure.facecolor'] = 'black'\n",
        "\n",
        "sample_indices = np.arange(len(yf))\n",
        "plt.figure(figsize=(20,5))\n",
        "# Plotting line graph for output 1\n",
        "plt.plot(sample_indices, yf['true_output_1'], label='Systolic(Actual)', linestyle='-', color='blue')\n",
        "plt.plot(sample_indices, yf['predicted_output_1'], label='Systolic(Predicted)', linestyle='--', color='cyan')\n",
        "\n",
        "# Plotting line graph for output 2\n",
        "plt.plot(sample_indices, yf['true_output_2'], label='Diastolic(Actual)', linestyle='-', color='green')\n",
        "plt.plot(sample_indices, yf['predicted_output_2'], label='Diastolic(Predicted)', linestyle='--', color='lime')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Sample Index',color='white')\n",
        "plt.ylabel('Output Values',color='white')\n",
        "plt.title('Comparison of Predicted and True Outputs',color='white')\n",
        "legend = plt.legend()\n",
        "for text in legend.get_texts():\n",
        "    text.set_color(\"white\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CxypOZ1xOP53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "icZAHVQQOvmn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}