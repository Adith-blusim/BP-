{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adith-blusim/BP-/blob/main/BPmodel2CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mir7qqvVddw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Minute-wise data labelling**"
      ],
      "metadata": {
        "id": "uGLx9tcBgIOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('mat_15.csv')\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'])  # Convert timestamp to datetime format\n",
        "df.set_index('Timestamp', inplace=True)\n",
        "# Resample the data to count occurrences per minute\n",
        "count_per_minute = df.resample('T').size()\n",
        "count_per_minute"
      ],
      "metadata": {
        "id": "-OsHxYmjaAHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rHXmGcMq8Yg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/blusim/roi_updated_data.csv\")\n",
        "df = df.dropna()\n",
        "df = df.reset_index(drop=True)\n",
        "df = df.drop(['Unix Timestamp','RESP','ECG','SPO2','PR'],axis=1)\n",
        "\n",
        "def extract_nibp_values(value):\n",
        "    # Remove spaces and extra brackets, extract the digits\n",
        "    cleaned_value = ''.join(filter(str.isdigit, value.replace(' ', '').replace('(', '').replace(')', '')))\n",
        "    sys_value = cleaned_value[:3]\n",
        "    dias_value = cleaned_value[3:5]\n",
        "    return sys_value, dias_value\n",
        "\n",
        "# Apply the function to \"NIBP\" column and create new columns \"sys\" and \"dias\"\n",
        "df['sys'], df['dias'] = zip(*df['NIBP'].apply(extract_nibp_values))\n",
        "\n",
        "rdf = pd.read_csv(\"/content/drive/MyDrive/blusim/mat_15.csv\")\n",
        "rdf['Timestamp'] = pd.to_datetime(rdf['Timestamp'], format='%H:%M:%S')\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%H:%M:%S')\n",
        "rdf['Timestamp'] = rdf['Timestamp'].dt.strftime('%H:%M')\n",
        "\n",
        "df.set_index('Timestamp', inplace=True)\n",
        "rdf = rdf.groupby('Timestamp').agg(lambda x: x.tolist()).reset_index()\n",
        "rdf.set_index('Timestamp', inplace=True)\n",
        "df=df.resample('T').ffill()\n",
        "df.index = df.index.strftime('%H:%M')\n",
        "\n",
        "ldf = pd.merge(rdf,df, left_index=True, right_index=True)\n",
        "ldf[['sys', 'dias']] = ldf[['sys', 'dias']].replace('', np.nan)\n",
        "ldf.dropna(how='any', inplace=True)\n",
        "ldf.drop(\"16:14\", inplace=True)\n",
        "ldf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Deleting all values of \"sys\" greater than 121:\n",
        "#seconds data\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/blusim/roi_updated_data.csv\")\n",
        "df = df.dropna()\n",
        "df = df.reset_index(drop=True)\n",
        "df = df.drop(['Unix Timestamp','RESP','ECG','SPO2','PR'],axis=1)\n",
        "\n",
        "def extract_nibp_values(value):\n",
        "    # Remove spaces and extra brackets, extract the digits\n",
        "    cleaned_value = ''.join(filter(str.isdigit, value.replace(' ', '').replace('(', '').replace(')', '')))\n",
        "    sys_value = cleaned_value[:3]\n",
        "    dias_value = cleaned_value[3:5]\n",
        "    return sys_value, dias_value\n",
        "\n",
        "# Apply the function to \"NIBP\" column and create new columns \"sys\" and \"dias\"\n",
        "df['sys'], df['dias'] = zip(*df['NIBP'].apply(extract_nibp_values))\n",
        "\n",
        "rdf = pd.read_csv(\"/content/drive/MyDrive/blusim/mat_15.csv\")\n",
        "rdf['Timestamp'] = pd.to_datetime(rdf['Timestamp'], format='%H:%M:%S')\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%H:%M:%S')\n",
        "rdf['Timestamp'] = rdf['Timestamp'].dt.strftime('%H:%M:%S')\n",
        "\n",
        "df.set_index('Timestamp', inplace=True)\n",
        "rdf = rdf.groupby('Timestamp').agg(lambda x: x.tolist()).reset_index()\n",
        "rdf.set_index('Timestamp', inplace=True)\n",
        "df=df.resample('S').ffill()\n",
        "df.index = df.index.strftime('%H:%M:%S')\n",
        "\n",
        "ldf = pd.merge(rdf,df, left_index=True, right_index=True)\n",
        "ldf[['sys', 'dias']] = ldf[['sys', 'dias']].replace('', np.nan)\n",
        "ldf.dropna(how='any', inplace=True)\n",
        "ldf['sys'] = pd.to_numeric(ldf['sys'], errors='coerce')\n",
        "ldf = ldf.drop(ldf[ldf['sys'].gt(121)].index)\n",
        "ldf"
      ],
      "metadata": {
        "id": "F5hmabuBac47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCHIP"
      ],
      "metadata": {
        "id": "i_wggMcXjWRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.interpolate import PchipInterpolator\n",
        "\n",
        "def interpolate_to_120(arr):\n",
        "\n",
        "    pchip_interpolator = PchipInterpolator(np.arange(len(arr)), arr)\n",
        "    x_interp = np.linspace(0, len(arr) - 1, 120)\n",
        "    y_interp = pchip_interpolator(x_interp)\n",
        "    return y_interp.tolist()\n",
        "\n",
        "%timeit df_interpolated = ldf[['Raw_Value_ch1','Raw_Value_ch2','Raw_Value_ch3','Raw_Value_ch4']].applymap(interpolate_to_120)\n",
        "df_interpolated"
      ],
      "metadata": {
        "id": "YfDBD01bgBOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cubic Spline"
      ],
      "metadata": {
        "id": "r3NOn4M0jTss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.interpolate import CubicSpline\n",
        "def interpolate_to_120(arr):\n",
        "\n",
        "    cs_interpolator = CubicSpline(np.arange(len(arr)), arr, bc_type='clamped')\n",
        "    x_interp = np.linspace(0, len(arr) - 1, 120)\n",
        "    y_interp = cs_interpolator(x_interp)\n",
        "    return y_interp.tolist()\n",
        "%timeit df_interpolated = ldf[['Raw_Value_ch1','Raw_Value_ch2','Raw_Value_ch3','Raw_Value_ch4']].applymap(interpolate_to_120)\n",
        "df_interpolated"
      ],
      "metadata": {
        "id": "hVbnUFDIiN3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.interpolate import CubicSpline\n",
        "def interpolate_to_120(arr):\n",
        "\n",
        "    x_interp = np.linspace(0, len(arr) - 1, 120)\n",
        "    y_interp = np.interp(x_interp, np.arange(len(arr)), arr)\n",
        "    return y_interp.tolist()\n",
        "%timeit df_interpolated = ldf[['Raw_Value_ch1','Raw_Value_ch2','Raw_Value_ch3','Raw_Value_ch4']].applymap(interpolate_to_120)\n",
        "df_interpolated"
      ],
      "metadata": {
        "id": "MgME4TJVi-J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_interpolated['Raw_Value_ch2'][45])"
      ],
      "metadata": {
        "id": "WTRMkheqg8T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Signal Processing Part**"
      ],
      "metadata": {
        "id": "sWbyystiV0w8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Butterworth"
      ],
      "metadata": {
        "id": "eTEqGOOsYhqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install EMD-signal\n",
        "from PyEMD import EMD\n",
        "from PyEMD import EEMD\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "# Sample rate and desired cutoff frequencies (in Hz)\n",
        "fs = 138.0\n",
        "lowcut = 0.5\n",
        "highcut = 6.0\n",
        "\n",
        "def butter_bandpass(lowcut, highcut, fs, order=3):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=3):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "# Channel_1 = ldf['Raw_Value_ch1']\n",
        "# Channel_2 = ldf['Raw_Value_ch2']\n",
        "# Channel_3 = ldf['Raw_Value_ch3']\n",
        "# Channel_4 = ldf['Raw_Value_ch4']\n",
        "\n",
        "# Apply the Butterworth band-pass filter to the data\n",
        "bdf = pd.DataFrame()\n",
        "bdf['buttered_channel1'] = [butter_bandpass_filter(data, lowcut, highcut, fs, order=3) for data in ldf['Raw_Value_ch1']]\n",
        "bdf['buttered_channel2'] = [butter_bandpass_filter(data, lowcut, highcut, fs, order=3) for data in ldf['Raw_Value_ch2']]\n",
        "bdf['buttered_channel3'] = [butter_bandpass_filter(data, lowcut, highcut, fs, order=3) for data in ldf['Raw_Value_ch3']]\n",
        "bdf['buttered_channel4'] = [butter_bandpass_filter(data, lowcut, highcut, fs, order=3) for data in ldf['Raw_Value_ch4']]\n",
        "\n",
        "# print(len(buttered_channel2[1]))\n",
        "bdf"
      ],
      "metadata": {
        "id": "_8VatXArVLE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EMD"
      ],
      "metadata": {
        "id": "VenCXhwPlWNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eemd = EMD()\n",
        "imf1c1 = []\n",
        "imf1c2 = []\n",
        "imf1c3 = []\n",
        "imf1c4 = []\n",
        "\n",
        "for i in bdf['buttered_channel1']:\n",
        "    time = np.arange(0, len(i)) / fs\n",
        "    IMFs = eemd.emd(i, time)\n",
        "    i1 = IMFs[0]\n",
        "    imf1c1.append(i1)\n",
        "\n",
        "for i in bdf['buttered_channel2']:\n",
        "    time = np.arange(0, len(i)) / fs\n",
        "    IMFs = eemd.emd(i, time)\n",
        "    i1 = IMFs[0]\n",
        "    imf1c2.append(i1)\n",
        "\n",
        "for i in bdf['buttered_channel3']:\n",
        "    time = np.arange(0, len(i)) / fs\n",
        "    IMFs = eemd.emd(i, time)\n",
        "    i1 = IMFs[0]\n",
        "    imf1c3.append(i1)\n",
        "\n",
        "for i in bdf['buttered_channel4']:\n",
        "    time = np.arange(0, len(i)) / fs\n",
        "    IMFs = eemd.emd(i, time)\n",
        "    i1 = IMFs[0]\n",
        "    imf1c4.append(i1)\n",
        "\n",
        "# print(len(imf1c4[1]))\n",
        "edf = pd.DataFrame()\n",
        "edf['IMF1_channel1'] = imf1c1\n",
        "edf['IMF1_channel2'] = imf1c2\n",
        "edf['IMF1_channel3'] = imf1c3\n",
        "edf['IMF1_channel4'] = imf1c4\n",
        "edf"
      ],
      "metadata": {
        "id": "17Lr6gcCllLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hilbert Transform"
      ],
      "metadata": {
        "id": "jZDSazuHo7BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal import hilbert\n",
        "ins_phase_c1 = []\n",
        "ins_phase_c2 = []\n",
        "ins_phase_c3 = []\n",
        "ins_phase_c4 = []\n",
        "\n",
        "for i in imf1c1:\n",
        "    analytic_signal = hilbert(i)\n",
        "    instantaneous_phase = np.angle(analytic_signal)\n",
        "    ins_phase_c1.append(instantaneous_phase)\n",
        "\n",
        "for i in imf1c2:\n",
        "    analytic_signal = hilbert(i)\n",
        "    instantaneous_phase = np.angle(analytic_signal)\n",
        "    ins_phase_c2.append(instantaneous_phase)\n",
        "\n",
        "for i in imf1c3:\n",
        "    analytic_signal = hilbert(i)\n",
        "    instantaneous_phase = np.angle(analytic_signal)\n",
        "    ins_phase_c3.append(instantaneous_phase)\n",
        "\n",
        "for i in imf1c4:\n",
        "    analytic_signal = hilbert(i)\n",
        "    instantaneous_phase = np.angle(analytic_signal)\n",
        "    ins_phase_c4.append(instantaneous_phase)\n",
        "\n",
        "hdf = pd.DataFrame()\n",
        "hdf['instphase_channel1'] = ins_phase_c1\n",
        "hdf['instphase_channel2'] = ins_phase_c2\n",
        "hdf['instphase_channel3'] = ins_phase_c3\n",
        "hdf['instphase_channel4'] = ins_phase_c4\n",
        "hdf"
      ],
      "metadata": {
        "id": "M_-3JbOjnsaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NN Model Input Design"
      ],
      "metadata": {
        "id": "racOhHeIjv8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mn = 5000\n",
        "for x in ins_phase_c2:\n",
        "    if len(x)<mn:\n",
        "      mn=len(x)\n",
        "print(mn)\n",
        "# max(len(ins_phase_c2))\n",
        "# max(len(hdf['instphase_channel1']))\n",
        "\n",
        "# standarizing array-size in each cell to be the same as the size of the smallest array\n",
        "minlen = (min(len(x) for x in ins_phase_c1))\n",
        "ipc1 = [lst[:minlen] for lst in ins_phase_c1 if len(lst) >= minlen]\n",
        "ipc2 = [lst[:minlen] for lst in ins_phase_c2 if len(lst) >= minlen]\n",
        "ipc3 = [lst[:minlen] for lst in ins_phase_c3 if len(lst) >= minlen]\n",
        "ipc4 = [lst[:minlen] for lst in ins_phase_c4 if len(lst) >= minlen]\n",
        "# hdf = pd.DataFrame()\n",
        "# hdf['instphase_channel1'] = ipc1\n",
        "# hdf['instphase_channel2'] = ipc2\n",
        "# hdf['instphase_channel3'] = ipc3\n",
        "# hdf['instphase_channel4'] = ipc4\n",
        "print(max(len(x) for x in hdf['instphase_channel2']))\n",
        "print(max(len(x) for x in ipc1))"
      ],
      "metadata": {
        "id": "0udAFYmoob6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_ipc = np.stack((ipc1,ipc2,ipc3,ipc4), axis=1)\n",
        "Y_train = ldf[['sys', 'dias']].to_numpy().astype('float32')\n",
        "merged_ipc = np.swapaxes(merged_ipc, 1, 2)\n",
        "merged_ipc.shape\n",
        "\n",
        "# Y_train.shape\n",
        "# merged_ipc.dtype"
      ],
      "metadata": {
        "id": "sy8Av-x4sH3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a = np.load('merged_ipc.npy')\n",
        "# if a.all() == merged_ipc.all():\n",
        "#   print(\"YEs\")\n",
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "YHA3-I46PDgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train = merged_ipc\n",
        "Y_train = ldf[['sys']].to_numpy().astype('float32')\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "L1DsjBv8OK1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Flatten, Dense, BatchNormalization\n",
        "\n",
        "input_shape =  (64, 4)\n",
        "model = Sequential()\n",
        "\n",
        "#first Conv layer\n",
        "model.add(Conv1D(filters=100, kernel_size=21, strides=1, input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.ReLU())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "#second Conv layer\n",
        "model.add(Conv1D(filters=200, kernel_size=5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.ReLU())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "#third Conv layer\n",
        "model.add(Conv1D(filters=300, kernel_size=5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.ReLU())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "#GlobalAveragePooling1D layer\n",
        "model.add(GlobalAveragePooling1D())\n",
        "#Final dense layer\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "model.compile(loss=tf.keras.losses.mean_squared_error, optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "model.summary()\n",
        "history = model.fit(X_train, Y_train, epochs=50, validation_data=(X_test,Y_test), batch_size=64)"
      ],
      "metadata": {
        "id": "OMvUV4TIJ5Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Training loss\n",
        "plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
        "\n",
        "# Validation loss\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Performance for input shape (1650,64,4), no interpolation')\n",
        "plt.legend()\n",
        "\n",
        "# Adding grid for better readability\n",
        "plt.grid(True)\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xxaq2i30yg1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_prediction = model.predict(X_test)\n",
        "# y_prediction = np.argmax (y_prediction, axis = 1)\n",
        "# y_test= np.argmax(Y_test, axis=1)\n",
        "# y_prediction\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "Y_pred = []\n",
        "for i in range(len(X_test)):\n",
        "  Y_pred.append(predictions[i])\n",
        "yf = pd.DataFrame(Y_pred, columns=['predicted'])\n",
        "yf['test-set'] = Y_test\n",
        "yf\n",
        "# Print the first few predictions and corresponding inputs\n",
        "# for i in range(min(5, len(X_test))):\n",
        "#     print(f\"Input: {X_test[i]}\")\n",
        "#     print(f\"Predicted Output: {predictions[i]}\")\n",
        "#     print(\"\\n\")"
      ],
      "metadata": {
        "id": "Mas399cPUnsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "y_pred = np.concatenate(Y_pred).ravel()\n",
        "mae = mean_absolute_error(Y_test, y_pred)\n",
        "mse = mean_squared_error(Y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "# Plot true vs predicted values\n",
        "plt.scatter(Y_test, y_pred)\n",
        "plt.xlabel(\"True Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"True vs Predicted Values\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Uq5kxz46m_3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Flatten, Dense, BatchNormalization\n",
        "\n",
        "\n",
        "X_train = merged_ipc\n",
        "Y_train2 = ldf[['dias']].to_numpy().astype('float32')\n",
        "X_train, X_test, Y_train, Y_test2 = train_test_split(X_train, Y_train2, test_size=0.2, random_state=42)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Flatten, Dense, BatchNormalization\n",
        "\n",
        "input_shape =  (64, 4)\n",
        "model2 = Sequential()\n",
        "\n",
        "#first Conv layer\n",
        "model2.add(Conv1D(filters=100, kernel_size=21, strides=1, input_shape=input_shape))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(tf.keras.layers.ReLU())\n",
        "model2.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "#second Conv layer\n",
        "model2.add(Conv1D(filters=200, kernel_size=5))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(tf.keras.layers.ReLU())\n",
        "model2.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "#third Conv layer\n",
        "model2.add(Conv1D(filters=300, kernel_size=5))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(tf.keras.layers.ReLU())\n",
        "model2.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "#GlobalAveragePooling1D layer\n",
        "model2.add(GlobalAveragePooling1D())\n",
        "#final dense layer\n",
        "model2.add(Dense(1, activation='linear'))\n",
        "\n",
        "model2.compile(loss=tf.keras.losses.mean_squared_error, optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "model2.summary()\n",
        "history2 = model2.fit(X_train, Y_train, epochs=50, validation_data=(X_test,Y_test2), batch_size=64)\n"
      ],
      "metadata": {
        "id": "NkM8GI9AUnwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Training loss\n",
        "plt.plot(history2.history['loss'], label='Training Loss', marker='o')\n",
        "\n",
        "# Validation loss\n",
        "plt.plot(history2.history['val_loss'], label='Validation Loss', marker='o')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Performance(dias) for input shape (1650,64,4), no interpolation')\n",
        "plt.legend()\n",
        "\n",
        "# Adding grid for better readability\n",
        "plt.grid(True)\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YqyLfBA_nL_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model2.predict(X_test)\n",
        "Y_pred2 = []\n",
        "for i in range(len(X_test)):\n",
        "  Y_pred2.append(predictions[i])\n",
        "yf['predicted-dias'] = Y_pred2\n",
        "yf['test-set-dias'] = Y_test2\n",
        "print(yf['predicted-dias'].dtype)\n",
        "yf"
      ],
      "metadata": {
        "id": "XRsu-2khhhU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating Mean of input data\n",
        "hdf['mean1'] = hdf['instphase_channel1'].apply(lambda arr: np.mean(arr) if len(arr) > 0 else np.nan)\n",
        "hdf['mean2'] = hdf['instphase_channel2'].apply(lambda arr: np.mean(arr) if len(arr) > 0 else np.nan)\n",
        "hdf['mean3'] = hdf['instphase_channel3'].apply(lambda arr: np.mean(arr) if len(arr) > 0 else np.nan)\n",
        "hdf['mean4'] = hdf['instphase_channel4'].apply(lambda arr: np.mean(arr) if len(arr) > 0 else np.nan)\n",
        "hdf\n",
        "# meaninput = hdf[['mean1','mean2','mean3','mean4']].to_numpy()\n",
        "# mergedinput = df.to_numpy()\n",
        "# mergedinput.shape"
      ],
      "metadata": {
        "id": "NqHsILRT_ql-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###########################################\n"
      ],
      "metadata": {
        "id": "s8vHxv_2gPQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#######################################################"
      ],
      "metadata": {
        "id": "vfZQCCCNgSmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#############################################################################################\n"
      ],
      "metadata": {
        "id": "tCq_ofXZ3N1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/blusim/mat_15.csv')\n",
        "df.head(150)"
      ],
      "metadata": {
        "id": "p3PG4kHFYRbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row_count_by_timestamp = df['Timestamp'][0].value_counts()\n",
        "# Print the results\n",
        "print(\"Number of rows for each timestamp:\")\n",
        "print((row_count_by_timestamp))"
      ],
      "metadata": {
        "id": "hPgqWLx7X5Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdf = pd.read_csv(\"/content/drive/MyDrive/blusim/mat_15.csv\")\n",
        "rdf['Timestamp'] = pd.to_datetime(rdf['Timestamp'], format='%H:%M:%S')\n",
        "rdf['Timestamp'] = rdf['Timestamp'].dt.strftime('%H:%M:%S')\n",
        "rdf = rdf.groupby('Timestamp').agg(lambda x: x.tolist()).reset_index()\n",
        "\n",
        "# Function to append next values to the first list\n",
        "def append_next_values(lst, next_values):\n",
        "    return lst[:1] + next_values\n",
        "\n",
        "# Iterate over the DataFrame and append next values to the first list\n",
        "for i in range(len(rdf) - 1):\n",
        "    next_values = rdf.iloc[i + 1, 1:].tolist()\n",
        "    for col in rdf.columns[1:]:\n",
        "        rdf.at[i, col] = append_next_values(rdf.at[i, col], next_values)\n",
        "\n",
        "# Pad the lists to a length of 100\n",
        "for col in rdf.columns[1:]:\n",
        "    rdf[col] = rdf[col].apply(lambda x: x + [None] * (100 - len(x)))\n",
        "\n",
        "# Print or use the modified DataFrame as needed\n",
        "rdf\n"
      ],
      "metadata": {
        "id": "gnvJEvUTWBzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gBUc6fiukKrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2iPsOiIDjANL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}